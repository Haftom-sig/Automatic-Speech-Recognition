{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949e6768",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition for Amharic Language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26dc3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title: Automatic Amharic Speech Recognition using  Deep Speech and CTC\n",
    "Authors:\n",
    "Date created: \n",
    "Last modified: \n",
    "Description: Training a CTC-based model for automatic speech recognition.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc40cd",
   "metadata": {},
   "source": [
    "## 1. Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe456c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from jiwer import wer\n",
    "import IPython.display as ipd\n",
    "from etnltk.lang.am import normalize\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c61bc",
   "metadata": {},
   "source": [
    "## 2. ASR Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ec85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename =r'../Data/ASR_arch.JPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33d69e0",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Load Amharic Dataset\n",
    "\n",
    "The dataset contains 13,100 audio files as `wav` files in the `/wavs/` folder.\n",
    "The label (transcript) for each audio file is a string\n",
    "given in the `metadata.csv` file. The fields are:\n",
    "\n",
    "- **ID**: this is the name of the corresponding .wav file\n",
    "- **Transcription**: words spoken by the reader (UTF-8)\n",
    "- **Normalized transcription**: transcription with numbers,\n",
    "ordinals, and monetary units expanded into full words (UTF-8).\n",
    "\n",
    "For this demo we will use on the \"Normalized transcription\" field.\n",
    "\n",
    "Each audio file is a single-channel 16-bit PCM WAV with a sample rate of 22,050 Hz.\n",
    "\"\"\"\n",
    "\n",
    "data_path = '../Data'\n",
    "wavs_path = data_path + \"/wav/\"\n",
    "metadata_path = data_path + \"/metadata_amh.csv\"\n",
    "\n",
    "# Read metadata file and parse it\n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "metadata_df.columns = [\"file_name\", \"transcription\"]\n",
    "metadata_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86b5424",
   "metadata": {},
   "source": [
    "## 4.Audio Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8347072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess_single_sample(wav_file):\n",
    "    ###########################################\n",
    "    ##  Process the Audio\n",
    "    ##########################################\n",
    "    # 1. Read wav file\n",
    "    file = tf.io.read_file(wavs_path + wav_file)    \n",
    "    # 2. Decode the wav file\n",
    "    audio, sample_rate = tf.audio.decode_wav(file,desired_channels=1)\n",
    "    fs = sample_rate.numpy()\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    return audio, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c8df72",
   "metadata": {},
   "source": [
    "### 4.1. Audio chunksize problems handling(special case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed929d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples are  taken out from index = 2539 to 3139 due to waveread error (chunk size  too short)\n",
    "metadata_new_df = pd.concat([metadata_df[0:2539],metadata_df[3140:len(metadata_df)]], axis = 0).reset_index(drop=True)\n",
    "metadata_new_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ff2bd",
   "metadata": {},
   "source": [
    "### 4.2.Check sample rate of all samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c6989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sample rate and duration of each audio sample in a given dataset\n",
    "def get_samplerate_duration(df): \n",
    "    sample_rate_ls = []\n",
    "    duration_ls = []\n",
    "    for i in range(len(df)):    \n",
    "        audio, sample_rate = Preprocess_single_sample(df['file_name'][i])\n",
    "        sample_rate_ls.append(sample_rate)\n",
    "        duration_ls.append(len(audio)/sample_rate)        \n",
    "    return sample_rate_ls,duration_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe971732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check sample rates of all samples\n",
    "samplerate_total,duration_total = get_samplerate_duration(metadata_new_df)\n",
    "plt.plot(samplerate_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b906ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Audio vs Transcription mapping\n",
    "index = 100 # randomly choosen number\n",
    "print(metadata_new_df['transcription'][index])\n",
    "ipd.Audio(wavs_path + metadata_new_df['file_name'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702e47d",
   "metadata": {},
   "source": [
    "### 4.3. Dataset duration analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c895ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The list of durations for each samples \n",
    "def show_duration_distribution(dr):\n",
    "    plt.hist(dr, bins = 10)\n",
    "    plt.show()  \n",
    "    # Note :\n",
    "    #The grapth shows sampels have different duration \n",
    "    #Cutting audio to fixed duration will cause trascription miss match\n",
    "    #Total Dataset duration\n",
    "    print(\"dataset duration is :\",sum(dr)/3600 ,\"Hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_duration_distribution(duration_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df66591",
   "metadata": {},
   "source": [
    "## 5.Dataset Train and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a788711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and split train test dataset\n",
    "metadata_new_df = shuffle(metadata_new_df).reset_index(drop=True)\n",
    "split = int(len(metadata_new_df) * 0.90)\n",
    "df_train = metadata_new_df[:split]\n",
    "df_val = metadata_new_df[split:].reset_index(drop=True)\n",
    "print(f\"Size of the training set: {len(df_train)}\")\n",
    "print(f\"Size of the training set: {len(df_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c2761",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_tr , t_tr = get_samplerate_duration(df_train)\n",
    "show_duration_distribution(t_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_ts , t_ts = get_samplerate_duration(df_val)\n",
    "show_duration_distribution(t_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check Audio vs Transcription mapping  \n",
    "indextr = 100 # randomly choosen number\n",
    "print(df_train['transcription'][indextr])\n",
    "ipd.Audio(wavs_path + df_train['file_name'][indextr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check Audio vs Transcription mapping\n",
    "indexts = 100 # randomly choosen number\n",
    "print(df_val['transcription'][indexts])\n",
    "ipd.Audio(wavs_path + df_val['file_name'][indexts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb38ff",
   "metadata": {},
   "source": [
    "## 6. Lable and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d55b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "Geez_char = \"\"\"\n",
    "ሀ ሁ ሂ ሄ ህ ሆ\n",
    "ለ ሉ ሊ ላ ሌ ል ሎ ሏ\n",
    "መ ሙ ሚ ማ ሜ ም ሞ ሟ\n",
    "ረ ሩ ሪ ራ ሬ ር ሮ ሯ\n",
    "ሰ ሱ ሲ ሳ ሴ ስ ሶ ሷ\n",
    "ሸ ሹ ሺ ሻ ሼ ሽ ሾ ሿ\n",
    "ቀ ቁ ቂ ቃ ቄ ቅ ቆ ቋ\n",
    "በ ቡ ቢ ባ ቤ ብ ቦ ቧ\n",
    "ቨ ቩ ቪ ቫ ቬ ቭ ቮ ቯ\n",
    "ተ ቱ ቲ ታ ቴ ት ቶ ቷ\n",
    "ቸ ቹ ቺ ቻ ቼ ች ቾ ቿ\n",
    "ኋ\n",
    "ነ ኑ ኒ ና ኔ ን ኖ ኗ\n",
    "ኘ ኙ ኚ ኛ ኜ ኝ ኞ ኟ\n",
    "አ ኡ ኢ ኤ እ ኦ ኧ\n",
    "ከ ኩ ኪ ካ ኬ ክ ኮ ኳ\n",
    "ወ ዉ ዊ ዋ ዌ ው ዎ\n",
    "ዘ ዙ ዚ ዛ ዜ ዝ ዞ ዟ\n",
    "ዠ ዡ ዢ ዣ ዤ ዥ ዦ ዧ\n",
    "የ ዩ ዪ ያ ዬ ይ ዮ\n",
    "ደ ዱ ዲ ዳ ዴ ድ ዶ ዷ\n",
    "ጀ ጁ ጂ ጃ ጄ ጅ ጆ ጇ\n",
    "ገ ጉ ጊ ጋ ጌ ግ ጐ ጓ ጔ\n",
    "ጠ ጡ ጢ ጣ ጤ ጥ ጦ ጧ\n",
    "ጨ ጩ ጪ ጫ ጬ ጭ ጮ ጯ\n",
    "ጰ ጱ ጲ ጳ ጴ ጵ ጶ ጷ\n",
    "ፀ ፁ ፂ ፃ ፄ ፅ ፆ ፇ\n",
    "ፈ ፉ ፊ ፋ ፌ ፍ ፎ ፏ\n",
    "ፐ ፑ ፒ ፓ ፔ ፕ ፖ\n",
    "። ፣ ? !\n",
    "\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e530b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [x for x in Geez_char]\n",
    "# Mapping characters to integers\n",
    "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\" \")\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = keras.layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), oov_token=\" \", invert=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
    "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ee545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def am_normalize_text(df):    \n",
    "    normlz_df = pd.DataFrame(columns=['normalized_transcription']) \n",
    "    for i in range(len(df)):\n",
    "        normlz_df.loc[i] = normalize(df.loc[i])\n",
    "    return normlz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next, we create the function that describes the transformation that we apply to each\n",
    "element of our dataset.\n",
    "\"\"\"\n",
    "\n",
    "# An integer scalar Tensor. The window length in samples.\n",
    "frame_length = 256\n",
    "# An integer scalar Tensor. The number of samples to step.\n",
    "frame_step = 160\n",
    "# An integer scalar Tensor. The size of the FFT to apply.\n",
    "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
    "fft_length = 384\n",
    "\n",
    "\n",
    "def encode_single_sample(wav_file, label):\n",
    "    ###########################################\n",
    "    ##  Process the Audio\n",
    "    ##########################################\n",
    "    # 1. Read wav file\n",
    "    file = tf.io.read_file(wavs_path + wav_file)\n",
    "    # 2. Decode the wav file\n",
    "    audio, _ = tf.audio.decode_wav(file)\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    # 3. Change type to float\n",
    "    audio = tf.cast(audio, tf.float32)\n",
    "    # 4. Get the spectrogram\n",
    "    spectrogram = tf.signal.stft(\n",
    "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
    "    )\n",
    "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
    "    # 6. normalisation\n",
    "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
    "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
    "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
    "    ###########################################\n",
    "    ##  Process the label\n",
    "    ##########################################\n",
    "    # 7.Normalize Amharic text\n",
    "    #label = normalize(label)\n",
    "    # 8. Split the label\n",
    "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")  #x.decode(\"UTF-8\") to decode the unicode\n",
    "    # 9. Map the characters in label to numbers\n",
    "    label = char_to_num(label)\n",
    "    # 10. Return a dict as our model is expecting two inputs\n",
    "    return spectrogram, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ee75c",
   "metadata": {},
   "source": [
    "## 7. Tensorflow Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f85038",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['normalized_transcription'] = am_normalize_text(df_train[\"transcription\"])\n",
    "df_val['normalized_transcription'] = am_normalize_text(df_val[\"transcription\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Creating `Dataset` objects\n",
    "\n",
    "We create a `tf.data.Dataset` object that yields\n",
    "the transformed elements, in the same order as they\n",
    "appeared in the input.\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 32\n",
    "# Define the trainig dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(df_train[\"file_name\"]), list(df_train[\"normalized_transcription\"]))\n",
    ")\n",
    "train_dataset = (\n",
    "    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Define the validation dataset\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (list(df_val[\"file_name\"]), list(df_val[\"normalized_transcription\"]))\n",
    ")\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .padded_batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d715f808",
   "metadata": {},
   "source": [
    "### 8.Audio Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Visualize the data\n",
    "\n",
    "Let's visualize an example in our dataset, including the\n",
    "audio clip, the spectrogram and the corresponding label.\n",
    "\"\"\"\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "for batch in train_dataset.take(1):\n",
    "    spectrogram = batch[0][0].numpy()\n",
    "    spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n",
    "    label = batch[1][0]\n",
    "    # Spectrogram\n",
    "    label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    ax.imshow(spectrogram, vmax=1)\n",
    "    #ax.set_title(label)\n",
    "    ax.axis(\"off\")\n",
    "    # Wav\n",
    "    file = tf.io.read_file(wavs_path + list(df_train[\"file_name\"])[0])\n",
    "    audio, _ = tf.audio.decode_wav(file)\n",
    "    audio = audio.numpy()\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    plt.plot(audio)\n",
    "    ax.set_title(\"Signal Wave\")\n",
    "    ax.set_xlim(0, len(audio))\n",
    "    display.display(display.Audio(np.transpose(audio), rate=16000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2e9eb",
   "metadata": {},
   "source": [
    "## 9. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218e652",
   "metadata": {},
   "source": [
    "### 9.1 CTC loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b80ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Model\n",
    "\n",
    "We first define the CTC Loss function.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def CTCLoss(y_true, y_pred):\n",
    "    # Compute the training-time loss value\n",
    "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c3541",
   "metadata": {},
   "source": [
    "### 9.2 DeepSpeech2 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We now define our model. We will define a model similar to\n",
    "[DeepSpeech2](https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/deepspeech2.html).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n",
    "    \"\"\"Model similar to DeepSpeech2.\"\"\"\n",
    "    # Model's input\n",
    "    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
    "    # Expand the dimension to use 2D CNN.\n",
    "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
    "    # Convolution layer 1\n",
    "    x = layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=[11, 41],\n",
    "        strides=[2, 2],\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        name=\"conv_1\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
    "    x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
    "    # Convolution layer 2\n",
    "    x = layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=[11, 21],\n",
    "        strides=[1, 2],\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        name=\"conv_2\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
    "    x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
    "    # Reshape the resulted volume to feed the RNNs layers\n",
    "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
    "    # RNN layers\n",
    "    for i in range(1, rnn_layers + 1):\n",
    "        recurrent = layers.GRU(\n",
    "            units=rnn_units,\n",
    "            activation=\"tanh\",\n",
    "            recurrent_activation=\"sigmoid\",\n",
    "            use_bias=True,\n",
    "            return_sequences=True,\n",
    "            reset_after=True,\n",
    "            name=f\"gru_{i}\",\n",
    "        )\n",
    "        x = layers.Bidirectional(\n",
    "            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
    "        )(x)\n",
    "        if i < rnn_layers:\n",
    "            x = layers.Dropout(rate=0.5)(x)\n",
    "    # Dense layer\n",
    "    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n",
    "    x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
    "    x = layers.Dropout(rate=0.5)(x)\n",
    "    # Classification layer\n",
    "    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
    "    # Model\n",
    "    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
    "    # Optimizer\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer=opt, loss=CTCLoss)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the model\n",
    "model = build_model(\n",
    "    input_dim=fft_length // 2 + 1,\n",
    "    output_dim=char_to_num.vocabulary_size(),\n",
    "    rnn_units=256,   #512\n",
    ")\n",
    "model.summary(line_length=110)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b36eba",
   "metadata": {},
   "source": [
    "### 9.3 Training and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4cf49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Training and Evaluating\n",
    "\"\"\"\n",
    "\n",
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for result in results:\n",
    "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(result)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "# A callback class to output a few transcriptions during training\n",
    "class CallbackEval(keras.callbacks.Callback):\n",
    "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs=None):\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        for batch in self.dataset:\n",
    "            X, y = batch\n",
    "            batch_predictions = model.predict(X)\n",
    "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
    "            predictions.extend(batch_predictions)\n",
    "            for label in y:\n",
    "                label = (\n",
    "                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "                )\n",
    "                targets.append(label)\n",
    "        wer_score = wer(targets, predictions)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Word Error Rate: {wer_score:.4f}\")\n",
    "        print(\"-\" * 100)\n",
    "        for i in np.random.randint(0, len(predictions), 2):\n",
    "            print(f\"Target    : {targets[i]}\")\n",
    "            print(f\"Prediction: {predictions[i]}\")\n",
    "            print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6432ab44",
   "metadata": {},
   "source": [
    "### 9.4 Start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's start the training process.\n",
    "\"\"\"\n",
    "\n",
    "# Define the number of epochs.\n",
    "epochs = 100\n",
    "# Callback function to check transcription on the val set.\n",
    "validation_callback = CallbackEval(validation_dataset)\n",
    "start_time = datetime.now()\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[validation_callback],\n",
    ")\n",
    "print('Duration: {}'.format(end_time - start_time))  \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aefd388",
   "metadata": {},
   "source": [
    "### 9.5 Check validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e240840",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Inference\n",
    "\"\"\"\n",
    "\n",
    "# Let's check results on more validation samples\n",
    "predictions = []\n",
    "targets = []\n",
    "for batch in validation_dataset:\n",
    "    X, y = batch\n",
    "    batch_predictions = model.predict(X)\n",
    "    batch_predictions = decode_batch_predictions(batch_predictions)\n",
    "    predictions.extend(batch_predictions)\n",
    "    for label in y:\n",
    "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
    "        targets.append(label)\n",
    "wer_score = wer(targets, predictions)\n",
    "print(\"-\" * 100)\n",
    "print(f\"Word Error Rate: {wer_score:.4f}\")\n",
    "print(\"-\" * 100)\n",
    "for i in np.random.randint(0, len(predictions), 5):\n",
    "    print(f\"Target    : {targets[i]}\")\n",
    "    print(f\"Prediction: {predictions[i]}\")\n",
    "    print(\"-\" * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea6d28",
   "metadata": {},
   "source": [
    "### 10.Save model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa58cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to pickle file\n",
    "with open('ASR_Amharic_epoch_100_(Aug_xx_22).pkl', 'wb') as files:\n",
    "     pickle.dump(model,files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eeaa3e",
   "metadata": {},
   "source": [
    "### 11.Plot model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892efb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('ASR_Amharic_epoch_100_(jul29_22).png')\n",
    "plt.show()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb0dfae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
